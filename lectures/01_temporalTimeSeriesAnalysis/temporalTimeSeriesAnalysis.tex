
\documentclass{beamer}

\hypersetup{colorlinks=true}

\mode<presentation> {

% The Beamer class comes with a number of default slide themes
% which change the colors and layouts of slides. Below this is a list
% of all the themes, uncomment each in turn to see what they look like.

%\usetheme{default}
%\usetheme{AnnArbor}
%\usetheme{Antibes}
%\usetheme{Bergen}
%\usetheme{Berkeley}
%\usetheme{Berlin}
%\usetheme{Boadilla}
%\usetheme{CambridgeUS}
%\usetheme{Copenhagen}
%\usetheme{Darmstadt}
%\usetheme{Dresden}
%\usetheme{Frankfurt}
%\usetheme{Goettingen}
%\usetheme{Hannover}
%\usetheme{Ilmenau}
%\usetheme{JuanLesPins}
%\usetheme{Luebeck}
\usetheme{Madrid}
%\usetheme{Malmoe}
%\usetheme{Marburg}
%\usetheme{Montpellier}
%\usetheme{PaloAlto}
%\usetheme{Pittsburgh}
%\usetheme{Rochester}
%\usetheme{Singapore}
%\usetheme{Szeged}
%\usetheme{Warsaw}

% As well as themes, the Beamer class has a number of color themes
% for any slide theme. Uncomment each of these in turn to see how it
% changes the colors of your current slide theme.

%\usecolortheme{albatross}
%\usecolortheme{beaver}
%\usecolortheme{beetle}
%\usecolortheme{crane}
%\usecolortheme{dolphin}
%\usecolortheme{dove}
%\usecolortheme{fly}
%\usecolortheme{lily}
%\usecolortheme{orchid}
%\usecolortheme{rose}
%\usecolortheme{seagull}
%\usecolortheme{seahorse}
%\usecolortheme{whale}
%\usecolortheme{wolverine}

%\setbeamertemplate{footline} % To remove the footer line in all slides uncomment this line
%\setbeamertemplate{footline}[page number] % To replace the footer line in all slides with a simple slide count uncomment this line

%\setbeamertemplate{navigation symbols}{} % To remove the navigation symbols from the bottom of all slides uncomment this line
}

% remove title and author from left panel
%  \makeatletter
%   \setbeamertemplate{sidebar \beamer@sidebarside}%{sidebar theme}
%   {
%     \beamer@tempdim=\beamer@sidebarwidth%
%     \advance\beamer@tempdim by -6pt%
%     \insertverticalnavigation{\beamer@sidebarwidth}%
%     \vfill
%     \ifx\beamer@sidebarside\beamer@lefttext%
%     \else%
%       \usebeamercolor{normal text}%
%       \llap{\usebeamertemplate***{navigation symbols}\hskip0.1cm}%
%       \vskip2pt%
%     \fi%
%   }%
% \makeatother
% done remove title and author from left panel 

\usepackage{graphicx} % Allows including images
\usepackage{booktabs} % Allows the use of \toprule, \midrule and \bottomrule in tables
\usepackage{natbib}
\usepackage{apalike}
\usepackage{comment}
\usepackage{listings,lstautogobble}
% \usepackage{enumitem}
% \setlist[itemize]{topsep=0pt,before=\leavevmode\vspace{-1.5em}}
% \setlist[description]{style=nextline}
\usepackage{amsthm}
\usepackage{media9}
% \usepackage{multimedia}
\usepackage{caption}
\usepackage{subcaption}
% \usepackage{hyperref}

% \usepackage{tikz}
% \tikzset{
%      arrow/.style={-{Stealth[]}}
%      }
% \usetikzlibrary{positioning,arrows.meta}
% \usetikzlibrary{shapes.geometric}

\setbeamertemplate{navigation symbols}{}%remove navigation symbols
\setbeamertemplate{caption}[numbered]%allow figure numbers

\usepackage{setspace}

% At the beginning put
%This custom command allows the number of frames to be stopped at this point
%(so later slides, eg. appendix do not appear as part of counter)
\newcommand{\beginappendix}{
   \newcounter{finalframenumber}
   \setcounter{finalframenumber}{\value{framenumber}}
}
\newcommand{\finishappendix}{
   \setcounter{framenumber}{\value{finalframenumber}}
}

\newtheorem{claim}{Claim}
\setbeamertemplate{theorems}[numbered]

\newenvironment<>{example1}[1][Example 1]{%
  \setbeamercolor{block title}{fg=white,bg=cyan!75!black}%
  \begin{block}{#1}}{\end{block}}
\newenvironment<>{example2}[1][Example 2]{%
  \setbeamercolor{block title}{fg=white,bg=magenta!75!black}%
  \begin{block}{#1}}{\end{block}}

\newcounter{saveenumi}
\newcommand{\seti}{\setcounter{saveenumi}{\value{enumi}}}
\newcommand{\conti}{\setcounter{enumi}{\value{saveenumi}}}
\newcommand{\keepi}{\addtocounter{saveenumi}{-1}\setcounter{enumi}{\value{saveenumi}}}

\DeclareMathOperator*{\argmin}{arg\,min}

%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\title{Temporal Time Series Analysis}

\author{Joaqu\'{i}n Rapela} % Your name
\institute[Gatsby Unit, UCL] % Your institution as it will appear on the bottom of every slide, may be shorthand to save space
{
Gatsby Computational Neuroscience Unit\\University College London % Your institution for the title page
}
\date{January 13, 2025} % Date, can be changed to a custom date

\AtBeginSection[]
  {
     \begin{frame}<beamer>
     \frametitle{Contents}
         \tableofcontents[currentsection,hideallsubsections]
     \end{frame}
  }

\AtBeginSubsection[]
  {
     \begin{frame}<beamer>
     \frametitle{Contents}
         \tableofcontents[currentsection,currentsubsection]
     \end{frame}
  }

\begin{document}

\begin{frame}
\titlepage % Print the title page as the first slide
\end{frame}

\begin{frame}
\frametitle{Contents} % Table of contents slide, comment this block out to remove it
\tableofcontents % Throughout your presentation, if you choose to use \section{} and \subsection{} commands, these will automatically be printed on this slide as an overview of your presentation
\end{frame}

\section{Course notes}

\begin{frame}
    \frametitle{Background}

    \begin{itemize}
        \item On Spring 2023 I helped in the discussion sessions of the
            Neuroinformatics course by Prof.~Ken Harris, for UCL undergraduate
            students in Neuroscience.

        \item Suggested to Klara Olofsdotter (SWC PhD program coordinator) and
            Sonja Hofer (SWC PhD program faculty coordinator) to ask SWC PhD
            students to take this course. They liked the idea.

        \item With Gatsby Unit PhD students and postdoctoral scholars, as well
            as researchers from elsewhere, we offered
            \href{https://github.com/joacorapela/neuroinformatics24}{Neuroinformatics
            2024},
            Ken taught the first five lectures and we taught the remaining
            ones.

        \item On 2025 we renamed the course \emph{Statistical Neuroscience} and
            we invited lecturers and students from the Francis Crick Institute.

    \end{itemize}

\end{frame}

\begin{frame}
\frametitle{A few of our motivations to run this course}

    \begin{enumerate}
        \item Learn by teaching.
        \item Gain more teaching experience.
        \item Provide SWC PhD students with relevant neural data-analysis tools.
        \item Contribute to better interactions between the SWC and the Gatsby
            Unit. Build a common language.
    \end{enumerate}

\end{frame}

\begin{frame}
\frametitle{Course structure}

    Refer to the course
    \href{https://github.com/joacorapela/statNeuro2025}{repo}.

\end{frame}

\begin{frame}
    \frametitle{Course logistics}

    \begin{description}

        \item[Lectures]: Monday 1-3pm, SWC lecture theatre.

        \item[Practicals]: Friday 2-3:30pm, SWC lecture theatre.

        \item[Office Hours]: Joaquin, Wednesday 4-5pm, or arranged by appointment.

        \item[Worksheets]: assigned on Mondays, due on the following Monday
            before 1pm. Worksheets by SWC PhD students will be graded. Solutions
            to all worksheets will be provided.

        \item[Participation]: in-class participation, and off-class
            participation (e.g., by email), is greatly encouraged.

    \end{description}

\end{frame}

\section{Time series analysis}

\subsection{Introduction to time series analysis}

\begin{frame}
    \frametitle{What is a time series?}

    \begin{definition}

        A \textbf{time series} is a set of observations $\{x_t\}$, each observation recorded at
        a specific time $t$.

    \end{definition}

\end{frame}


\begin{frame}
    \frametitle{Key concepts in probability theory}

    \begin{description}

        \item[sample space] The set, $\mathcal{S}$, of all possible outcomes of
            a particula experiment is called the \textbf{sample space} for this
            experiment.

        \item[event] An \textbf{event} is any collection of possible outcomes
            of an experiment, that is, any subset of $\mathcal{S}$ (including
            $\mathcal{S}$ itself).

        \item[probability function] A \textbf{probability function} is a
            function that assigns a real number (i.e., the probability) to
            events.

        \item[random variable] A \textbf{random variable} is a function from
            the sample space $\mathcal{S}$ into the real numbers. For example
            in an experiment tossing two dice a random variable X could be the
            sum of the obtained numbers, or in an experiment tossing a coin 25
            times a random variable Y could be the number of heads in the 25
            tosses.

            The probability that a random variable takes a given value is the
            probability of the event associated with that value.

            \begin{align*}
                P_X(X=x_i)=P(\{s_j\in\mathcal{S}:X(s_j)=x_i\})
            \end{align*}

    \end{description}

\end{frame}

\begin{frame}
    \frametitle{Time series model}

    The purpose of this course is to study techniques to draw inferences from
    neural time series. Before doing so, it is necessary to set up a
    hypothetical proability model to represent the time series. Then it is
    possible to estimate model parameters, check for goodness of fit to the
    data, and possibly use the estimated model to enhance our understanding of
    the mechanisms generating the time series.

    \begin{definition}[time series model]

        A \textbf{time series model} for the observed data $\{x_t\}$ is a
        specification of the joint distribution (or possibly the means and
        covariances) of a sequence of random variables $\{X_t\}$ of which
        $\{x_t\}$  is postulated to be a realization.

    \end{definition}

\end{frame}

\begin{frame}
    \frametitle{What is time series analysis?}

    \begin{itemize}

        \item Time series analysis characterises \textbf{data that is correlated in
            time}.

        \item These correlations severely \textbf{restrict the applicability of
            conventional techniques} assuming data samples that are
            independent and identically distributed.

        \item These correlations allow to \textbf{forecast} future values of a
            time series based on present and past values.

    \end{itemize}

\end{frame}

\begin{frame}
    \frametitle{Relevance of time series analysis}

    \begin{description}

        \item[economics] daily stock market quotations, monthly unemployment
            figures.

        \item[social scientists] birthrates, school enrolment.

        \item[epidemiology] number of influenza cases observed over some time
            period.

        \item[medicine] blood pressure measurements traced over time.

    \end{description}

\end{frame}

\begin{frame}
    \frametitle{Examples of SWC time series analysis}

    \begin{enumerate}
        \item \href{docs/inference.pdf}{aeon project: kinematic inference}.
            \begin{center}
                \href{videos/FrameTop_2021-06-03T17-00-00_end001000.avi}{\includegraphics[width=1.0in]{videos/firstFrmae_FrameTop_2021-06-03T17-00-00_end001000.jpg}}
            \end{center}

        \item \href{https://www.gatsby.ucl.ac.uk/~rapela/sepi/reports/kfAnalysis/expt01/expt01VisualVestibular.pdf}{integration
            of visual/vestibular information, with Prof.~Sepi Keshavarsi}.
            \begin{center}
                \includegraphics[width=3in]{figures/visVesIntegration.png}
            \end{center}
    \end{enumerate}

\end{frame}

\begin{frame}
    \frametitle{Temporal vs spectral time series analysis}

    \begin{description}

        \item[temporal time series analysis] focuses on the analysis of lagged
            relationship (e.g., how does what happened today affect what will
            happen tomorrow?).

        \item[spectral time series analysis] centres on the analysis of rhythms
            (e.g., can we observe rhythmic activity in local field potentials
            recorded from human brains?)

    \end{description}

\end{frame}

\subsection{Generation of time series}

\begin{frame}[fragile]
    \frametitle{Generation of time series: white noise process}
    \label{slide:autocovarWhiteNoise}

    The first step to generate time series is to generate \textbf{white noise process},
    $\{w_t\}$
    (i.e., independent Gaussian random variables with zero mean and fixed
    variance,
    \href{https://joacorapela.github.io/statNeuro2025/auto_examples/01_temporalTimeSeriesAnalysis/plot_whiteNoiseExample.html#sphx-glr-auto-examples-01-temporaltimeseriesanalysis-plot-whitenoiseexample-py}{example}).
    For $t=0,\pm1,\pm2,\ldots$
    \scriptsize
    \begin{align*}
        E\{w_t\}&=0\\
        \gamma(t, s)=\text{Cov}(w_t,w_s)&=\left\{\begin{array}{l l}
                                          \sigma^2_w & s=t\\
                                          0 & s\neq t\\
                                      \end{array}\right.
    \end{align*}
    \normalsize
    \begin{center}
        \href{http://www.gatsby.ucl.ac.uk/~rapela/statNeuro/2025/lectures/01_temporalTimeSeriesAnalysis/figures/whiteNoise.html}{\includegraphics[width=3in]{../../examples/sphinx_gallery/01_temporalTimeSeriesAnalysis/figures/whiteNoise.png}}
    \end{center}

\end{frame}

\begin{frame}[fragile]
    \frametitle{Generation of time series: moving average model}

    In a white noise process $w_t$, for any pairs of time points,
    $t_1$ and $t_2$, the random variables $w_{t_1}$ and $w_{t_2}$ are
    uncorrelated. The \textbf{moving average model} adds serial correlation to
    white noise 
    (\href{https://joacorapela.github.io/statNeuro2025/auto_examples/01_temporalTimeSeriesAnalysis/plot_movingAverageExample.html#sphx-glr-auto-examples-01-temporaltimeseriesanalysis-plot-movingaverageexample-py}{example}).

    \begin{align}
        \nu_t = \frac{1}{3}(w_{t-1} + w_t + w_{t+1})\label{eq:ma}
    \end{align}

    \begin{center}
        \href{http://www.gatsby.ucl.ac.uk/~rapela/statNeuro/2025/lectures/01_temporalTimeSeriesAnalysis/figures/movingAverage.html}{\includegraphics[width=2.75in]{../../examples/sphinx_gallery/01_temporalTimeSeriesAnalysis/figures/movingAverage.png}}
    \end{center}

\end{frame}

\begin{frame}[fragile]
    \frametitle{Moving average model of order $q$}

	\begin{definition}
		An \textbf{moving average model} of order $q$, abbreviated as \textbf{MA}(q), is of the form
		\begin{align*}
			x_t = w_t + \theta_1w_{t-1} + \theta_2w_{t-2} + \ldots + \theta_qw_{t-q}
		\end{align*}
        for $t=0,\pm1,\pm2,\ldots$, where $\{w_t\}$ is a white noise process
        and $\theta_1,\ldots,\theta_p$ are constants ($\theta_p\neq 0$).
	\end{definition}

\end{frame}

\begin{frame}[fragile]
    \frametitle{Generation of time series: autoregressive model}

    Many neural time series, like local field potential recordings, exhibit
    oscillations of the type of sine waves. The \textbf{autoregressive model}
    generates oscillations
    (\href{https://joacorapela.github.io/statNeuro2025/auto_examples/01_temporalTimeSeriesAnalysis/plot_autoregressiveExample.html#sphx-glr-auto-examples-01-temporaltimeseriesanalysis-plot-autoregressiveexample-py}{example}).

    \begin{align*}
        x_t = x_{t-1} - 0.9 x_{t-2} + w_t
    \end{align*}

    \begin{center}
        \href{http://www.gatsby.ucl.ac.uk/~rapela/statNeuro/2025/lectures/01_temporalTimeSeriesAnalysis/figures/autoregressive.html}{\includegraphics[width=2.75in]{../../examples/sphinx_gallery/01_temporalTimeSeriesAnalysis/figures/autoregressive.png}}
    \end{center}

\end{frame}

\begin{frame}[fragile]
    \frametitle{Autoregressive model of order $p$}

	\begin{definition}
		An \textbf{autoregressive model} of order $p$, abbreviated as \textbf{AR}(p), is of the form
		\begin{align*}
			x_t = \phi_1x_{t-1} + \phi_2x_{t-2} + \ldots + \phi_px_{t-p} + w_t
		\end{align*}
        for $t=0,\pm1,\pm2,\ldots$, where ${w_t}$ is a white noise process,
        $\phi_1,\ldots,\phi_p$ are constants ($\phi_p\neq 0$).
	\end{definition}

\end{frame}

\begin{frame}[fragile]
    \frametitle{Generation of time series: random walk with drift}

    The \textbf{random walk with noise} model is used to characterise trends in
    time series
    (\href{https://joacorapela.github.io/statNeuro2025/auto_examples/01_temporalTimeSeriesAnalysis/plot_randomWalkWithDriftExample.html#sphx-glr-auto-examples-01-temporaltimeseriesanalysis-plot-randomwalkwithdriftexample-py}{example}).

    \begin{align}
        x_t = \delta + x_{t-1} + w_t\label{eq:randomWalkWithDrift}
    \end{align}

    for $t=1,2,\ldots$, with initial condition $x_0=0$, and where $\{w_t\}$ is
    a white noise random process.
    \begin{center}
        \href{http://www.gatsby.ucl.ac.uk/~rapela/statNeuro/2025/lectures/01_temporalTimeSeriesAnalysis/figures/randomWalkWithDrift.html}{\includegraphics[width=2.75in]{../../examples/sphinx_gallery/01_temporalTimeSeriesAnalysis/figures/randomWalkWithDrift.png}}
        \label{fig:randomWalkWithDrift}
    \end{center}

\end{frame}

\begin{frame}[fragile]
    \frametitle{Generation of time series: signal plus noise}

    Many realistic models of time series assume an underlying signal with a
    periodic variation contaminated by adding a random noise
    (\href{https://joacorapela.github.io/statNeuro2025/auto_examples/01_temporalTimeSeriesAnalysis/plot_signalPlusNoise.html#sphx-glr-auto-examples-01-temporaltimeseriesanalysis-plot-signalplusnoise-py}{example}).

    \begin{align*}
        x_t &= 2\cos(2\pi\frac{t}{50}+2\pi\frac{15}{50})+w_t
    \end{align*}

    \begin{align*}
        A\cos(2\pi\omega t+\phi)
    \end{align*}

    for $t=0,\pm1,\pm2,\ldots$, where $A=2,\omega=1/50,\phi=2\pi 15/50$.
    \textcolor{red}{Lecture on spectral analysis of time series}.

    \begin{center}
        \href{http://www.gatsby.ucl.ac.uk/~rapela/statNeuro/2025/lectures/01_temporalTimeSeriesAnalysis/figures/signalPlusNoise.html}{\includegraphics[width=2.00in]{../../examples/sphinx_gallery/01_temporalTimeSeriesAnalysis/figures/signalPlusNoise.png}}
    \end{center}

\end{frame}

\subsection{Population measures used to describe time series}

\begin{frame}
    \frametitle{Mean function}

    \begin{definition}[Mean function]
        The mean function, $\mu_t$,  is defined as $\mu_t = E\{x_t\}$.
    \end{definition}

    \begin{exampleblock}{Example (Mean function of a moving average model)}
        \label{slide:ma}
        Calculate the mean function of the moving average model in Eq.~\ref{eq:ma}.
        \begin{align*}
            E\{\nu_t\} = \frac{1}{3}(E\{w_{t-1}\} + E\{w_t\} + E\{w_{t+1}\}) = \frac{1}{3}(0 + 0 + 0) = 0
        \end{align*}

    \end{exampleblock}

\end{frame}

\begin{frame}
    \frametitle{Mean function}

    \begin{exampleblock}{Example (Mean function of the autoregressive model of
        order 1)}
        Calculate the mean function of the autoregressive model of order 1,
        AR(1), in Eq.~\ref{eq:ar1}.
        \begin{align}
            x_t = \phi x_{t-1} + w_t\label{eq:ar1}
        \end{align}

        For $|\phi|<1$, the AR(1) model in Eq.~\ref{eq:ar1} can be represented as a moving average of infinite order MA($\infty$).
        See \hyperlink{slide:ar1AsMaInfty}{MA($\infty$) representation of AR(1) random process} in Appendix. Then
        \begin{align*}
            x_t &= \sum_{i=0}^{\infty}\phi^iw_{t-i}
        \end{align*}

        \begin{align*}
            E\{x_t\} &= \sum_{i=0}^{\infty}\phi^iE\{w_{t-i}\}=\sum_{i=0}^{\infty}\phi^i0=0
        \end{align*}

    \end{exampleblock}

\end{frame}

\begin{frame}
    \frametitle{Mean function}

    \begin{exampleblock}{Example (Mean function of the random walk with drift model)}
        Calculate the mean function of the random noise with drift  model, in Eq.~\ref{eq:randomWalkWithDrift}.

        The random noise with drift model in Eq.~\ref{eq:randomWalkWithDrift} can be represented as
        \begin{align*}
            x_t &= t \delta + \sum_{i=1}^tw_i
        \end{align*}

        For a proof see
        \hyperlink{slide:alternativeRepRandomWalkWithDrif}{Alternative
        representation of the random walk with drift} in the Appendix. Then
        \begin{align*}
            E\{x_t\} &= \delta t + \sum_{i=1}^tE\{w_i\} = \delta t + \sum_{i=1}^t0 = \delta t
        \end{align*}

        In the \hyperlink{fig:randomWalkWithDrift}{figure} of samples of the random noise with
        drift random process the dotted lines plot the mean of the process.

    \end{exampleblock}

\end{frame}

\begin{frame}
    \frametitle{Autocovariance function}

    \begin{definition}[Autocovariance function]
        The autocovariance function is defined as
        $\gamma(s, t)=cov(x_s,x_t)=E\{(x_s-\mu_s)(x_t-\mu_t)\}$.
    \end{definition}

    \begin{block}{Note}
        For $s=t$ the autocovariance function reduces to the variance function, because
        $\gamma(t,t)=E\{(x_t-\mu_t)^2\}=var(t)$.
    \end{block}

    \begin{definition}[Autocorrelation function]
        The autocorrelation function is defined as
        $\rho(s, t)=\frac{\gamma(t,s)}{\sqrt{\gamma(t,t)\gamma(s,s)}}$.
    \end{definition}

\end{frame}

\begin{frame}
    \frametitle{Autocovariance function}

    \label{slide:autocovarMA}
    \begin{exampleblock}{Example (Autocovariance function of moving average)}
		\only<1>{
        Calculate the autocovariance function of the moving average model in Eq.~\ref{eq:ma}.
		\begin{align*}
			\gamma_\nu(s,t)=cov(\nu_s,\nu_t)=cov(\frac{1}{3}(w_{s-1}+w_s+w_{s+1}),\frac{1}{3}(w_{t-1}+w_t+w_{t+1}))
		\end{align*}
		If s=t:
		\begin{align*}
			\gamma_\nu(t,t)&=cov(\nu_t,\nu_t)=cov(\frac{1}{3}(w_{t-1}+w_t+w_{t+1}),\frac{1}{3}(w_{t-1}+w_t+w_{t+1}))\\
			               &=\frac{1}{9}\left(cov(w_{t-1},w_{t-1})+cov(w_t,w_t)+cov(w_{t+1},w_{t+1})\right)\\
			               &=\frac{1}{9}\left(\sigma_w^2+\sigma_w^2+\sigma_2^2\right)=\frac{3}{9}\sigma_w^2
		\end{align*}
		}
		\only<2>{
		If s=t+1:
		\begin{align*}
			\gamma_\nu(t+1,t)&=cov(\nu_{t+1},\nu_t)\\
                             &=cov(\frac{1}{3}(w_t+w_{t+1}+w_{t+2}),\frac{1}{3}(w_{t-1}+w_t+w_{t+1}))\\
			                 &=\frac{1}{9}\left(cov(w_t,w_t)+cov(w_{t+1},w_{t+1})\right)\\
			                 &=\frac{1}{9}\left(\sigma_w^2+\sigma_w^2\right)=\frac{2}{9}\sigma_w^2
		\end{align*}
		}
		\only<3>{
		If s=t+2:
		\begin{align*}
			\gamma_\nu(t+2,t)&=cov(\nu_{t+2},\nu_t)\\
                             &=cov(\frac{1}{3}(w_{t+1}++w_{t+2}+w_{t+3}),\frac{1}{3}(w_{t-1}+w_t+w_{t+1}))\\
			                 &=\frac{1}{9}\left(cov(w_{t+1},w_{t+1})\right)\\
			                 &=\frac{1}{9}\sigma_w^2
		\end{align*}
		}
		\only<4>{
		\begin{align*}
        	\gamma_\nu(s,t)=\left\{\begin{array}{l l}
                                       \frac{3}{9}\sigma^2_w & \text{if}\quad s=t,\\
                                       \frac{2}{9}\sigma^2_w & \text{if}\quad |s-t|=1,\\
                                       \frac{1}{9}\sigma^2_w & \text{if}\quad |s-t|=2,\\
                                       0                     & \text{if}\quad  |s-t|>2.
                                   \end{array}\right.
		\end{align*}
		}
    \end{exampleblock}
\end{frame}

\begin{frame}
    \frametitle{Autocovariance function}

    \begin{exampleblock}{Example (Autocovariance function of AR(1))}
        Calculate the autocovariance function of the autoregressive model
        of order 1 in Eq.~\ref{eq:ar}.

        For $|\phi|<1$, an AR(1) model (Eq.~\ref{eq:ar1}) can be represented as a moving average of infinite order MA($\infty$).
        See \hyperlink{slide:ar1AsMaInfty}{MA($\infty$) representation of AR(1) random process} in Appendix.
        \begin{align*}
            x_t &= \sum_{i=0}^{\infty}\phi^iw_{t-i}
        \end{align*}
        \normalsize
    \end{exampleblock}
\end{frame}

\begin{frame}
    \frametitle{Autocovariance function}

    \begin{exampleblock}{Example (Autocovariance function of AR(1))}
        \label{slide:autocovarAR1}
        \scriptsize
        \begin{align}
            \gamma(t-h,t)&=E\{(x_{t-h}-\mu_{t-h})(x_t-\mu_t)\}=E\{x_{t-h}x_t\}=E\left\{\left(\sum_{i=0}^{\infty}\phi^iw_{t-h-i}\right)\left(\sum_{j=0}^{\infty}\phi^jw_{t-j}\right)\right\}\label{eq:autocovarAR1_l1}\\
                       &=E\left\{\sum_{i=0}^{\infty}\sum_{j=0}^{\infty}\phi^i\phi^jw_{t-h-i}w_{t-j}\right\}=\sum_{i=0}^{\infty}\sum_{j=0}^{\infty}\phi^i\phi^jE\{w_{t-h-i}w_{t-j}\}\label{eq:autocovarAR1_l2}\\
                       &=\sum_{i=0}^{\infty}\phi^i\phi^{i+h}E\{w_{t-h-i}^2\}=\phi^h\sigma^2_w\sum_{i=0}^{\infty}\phi^{2i}=\phi^h\sigma^2_w\frac{1}{1-\phi^2}\label{eq:autocovarAR1_l3}\qed
        \end{align}
    \end{exampleblock}
\end{frame}

\begin{frame}
    \frametitle{Autocovariance function}

    \begin{exampleblock}{Example (Autocovariance function of AR(1))}
        \begin{block}{Note}
            \begin{enumerate}
                \item the last equality in Eq.~\ref{eq:autocovarAR1_l1}
                    requires $|\phi|<1$ to use the
                    \hyperlink{slide:ar1AsMaInfty}{MA($\infty$) representation
                    of AR(1) random process}, which guarantees that
                    $\sum_{i=0}^\infty\phi^{2i}=\frac{1}{1-\phi^2}$ in
                    Eq.~\ref{eq:autocovarAR1_l3}.
                \item because the noise $w_t$ is uncorrelated, the expectation
                    in the right hand side of 
                    Eq.~\ref{eq:autocovarAR1_l2} will be different from zero
                    only when $t-h-i=t-j$, or when $j=h+i$. Thus, the double
                    summation in this equation will reduce to a single
                    summation replacing $j$ by $h+i$ in the lefmost summation
                    in Eq.~\ref{eq:autocovarAR1_l3}.
            \end{enumerate}
        \end{block}
        \normalsize
    \end{exampleblock}
\end{frame}

\subsection{Stationarity}

\begin{frame}
    \frametitle{Strictly stationary time series}

    \begin{definition}[Strict stationarity]
        \small
        A \textbf{strictly stationary time series} is one for which the probabilistic
        behaviour of every collection of values

        \begin{align*}
            \{x_{t_1},\ldots,x_{t_n}\}
        \end{align*}

        is identical to that of any shifted set 

        \begin{align*}
            \{x_{t_1+h},\ldots,x_{t_n+h}\}
        \end{align*}

        That is

        \begin{align*}
            P(x_{t_1}<c_1,\ldots,x_{t_k}<c_k)=P(x_{t_1+h}<c_1,\ldots,x_{t_k+h}<c_k)
        \end{align*}

        for all $k=1,2,\ldots$, all time points $t_1,t_2,\ldots,t_k$, all
        numbers $c_1,c_2,\ldots,c_k$, and all time shifts $h=0,\pm 1,\pm
        2,\ldots$.
        \normalsize
    \end{definition}

\end{frame}

\begin{frame}
    \frametitle{Weakly stationary time series}

    \begin{definition}[Weak or wide-sense stationarity]

        A \textbf{weakly} or \textbf{wide-sense stationary time series} is a
        finite-variance process such that:

        \begin{enumerate}[i]

            \item the mean function, $\mu_t$, is constant and does not depend on
                time $t$, and

            \item the autocovariance function, $\gamma(s,t)$, depends on $s$
                and $t$ only through their difference $|s-t|$.

        \end{enumerate}
    \end{definition}

\end{frame}

\subsection{Sample measures used to describe time series}

\begin{frame}
    \frametitle{Sample mean, autocovariance and autocorrelation}
    \label{slide:estAutocovar}

    \only<1>{
    \begin{definition}[Sample mean]
        Let $x_1,\ldots,x_n$ be observations from a time series. The
        \textbf{sample mean} of $x_1,\ldots,x_n$ is
        \begin{align*}
            \bar{x}=\frac{1}{n}\sum_{i=1}^nx_i
        \end{align*}
    \end{definition}

    \begin{definition}[Sample autocovariance]
        The \textbf{sample autocovariance function} is
        \begin{align*}
            \hat\gamma(h)=\frac{1}{n}\sum_{i=1}^{n-|h|}(x_{i+|h|}-\bar{x})(x_i-\bar{x}),\quad -n<h<n
        \end{align*}
    \end{definition}
    }

    \only<2>{
    \begin{definition}[Sample autocorrelation]
        The \textbf{sample autocorrelation function} is
        \begin{align*}
            \hat\rho(h)=\frac{\hat\gamma(h)}{\hat\gamma(0)}
        \end{align*}
    \end{definition}
    \begin{theorem}[Distribution of sample autocorrelation for white noise]

        For white noise, and a sample of size $n$,  the sample
        autocorrelations, $\hat\gamma(h), h>0$, are approximately independent
        and identically distributed (with a Normal distribution of mean 0 and
        standard deviaiton $1/\sqrt{n}$), for large $n$
        \citep{brockwellAndDavis91}.
        %
        Hence 95\% of the sample autocorrelations should fall between the bound
    $\pm 1.96/\sqrt{n}$ \end{theorem}

    }

\end{frame}

\begin{frame}
    \frametitle{Analytical and estimated autocorrelation function for white
    noise}

    Simulate a \hyperlink{slide:whiteNoise}{white noise} time series with N=100 and
    N=100,000 samples. For each N, plot the
    \hyperlink{slide:autocovarWhiteNoise}{analytical} and
    \hyperlink{slide:estAutocovar}{estimated} autocorrelation function.
    Include the 95\% confidence interval of the autocorrelation function.
    \href{https://joacorapela.github.io/statNeuro2025/auto_examples/01_temporalTimeSeriesAnalysis/plot_whiteNoiseAutocorrelation.html\#sphx-glr-auto-examples-01-temporaltimeseriesanalysis-plot-whitenoiseautocorrelation-py}{Solution}.

    \begin{center}
        \href{http://www.gatsby.ucl.ac.uk/~rapela/statNeuro/2025/lectures/01_temporalTimeSeriesAnalysis/figures/whiteNoiseAutoCorN100.html}{\includegraphics[width=2.00in]{../../examples/sphinx_gallery/01_temporalTimeSeriesAnalysis/figures/whiteNoiseAutoCorN100.png}}
        \href{http://www.gatsby.ucl.ac.uk/~rapela/statNeuro/2025/lectures/01_temporalTimeSeriesAnalysis/figures/whiteNoiseAutoCorN10000.html}{\includegraphics[width=2.00in]{../../examples/sphinx_gallery/01_temporalTimeSeriesAnalysis/figures/whiteNoiseAutoCorN10000.png}}
    \end{center}

\end{frame}

\begin{frame}
    \frametitle{Analytical and estimated autocovariance function for MA}

    Simulate the \hyperlink{slide:ma}{previous} moving average time series with N=100 and
    N=100,000 samples.  For each N, plot the
    \hyperlink{slide:autocovarMA}{analytical} and
    \hyperlink{slide:estAutocovar}{estimated} autocovariance function.
    \href{https://joacorapela.github.io/statNeuro2025/auto_examples/01_temporalTimeSeriesAnalysis/plot_movingAverageAutocovariance.html\#sphx-glr-auto-examples-01-temporaltimeseriesanalysis-plot-movingaverageautocovariance-py}{Solution}.

    \begin{center}
        \href{http://www.gatsby.ucl.ac.uk/~rapela/statNeuro/2025/lectures/01_temporalTimeSeriesAnalysis/figures/movingAverageAutoCovN100.html}{\includegraphics[width=2.00in]{../../../solutions/worksheets/01_temporalTimeSeriesAnalysis/figures/movingAverageAutoCovN100.png}}
        \href{http://www.gatsby.ucl.ac.uk/~rapela/statNeuro/2025/lectures/01_temporalTimeSeriesAnalysis/figures/movingAverageAutoCovN10000.html}{\includegraphics[width=2.00in]{../../../solutions/worksheets/01_temporalTimeSeriesAnalysis/figures/movingAverageAutoCovN10000.png}}
    \end{center}

\end{frame}

\begin{frame}
    \frametitle{Analytical and estimated autocovariance function for AR(1)}

    Simulate an AR(1) time series with N=100 and N=100,000 samples, $\phi=-0.9$
    and $\sigma_w=1.0$. For each N, plot the
    \hyperlink{slide:autocovarAR1}{analytical} and
    \hyperlink{slide:estAutocovar}{estimated} autocovariance function.

    \begin{center}
        \href{http://www.gatsby.ucl.ac.uk/~rapela/statNeuro/2025/lectures/01_temporalTimeSeriesAnalysis/figures/autoregressiveAutoCovN100.html}{\includegraphics[width=2.00in]{../../../solutions/worksheets/01_temporalTimeSeriesAnalysis/figures/autoregressiveAutoCovN100.png}}
        \href{http://www.gatsby.ucl.ac.uk/~rapela/statNeuro/2025/lectures/01_temporalTimeSeriesAnalysis/figures/autoregressiveAutoCovN10000.html}{\includegraphics[width=2.00in]{../../../solutions/worksheets/01_temporalTimeSeriesAnalysis/figures/autoregressiveAutoCovN10000.png}}
    \end{center}

\end{frame}

\subsection{Forecasting}

\begin{frame}
    \frametitle{Forecasting}

    Forecasting is the problem of predicting the value of $x_{n+h}, h>0$, of a
    stationary time series, in term of the previous $m$
    values$\{x_n,\ldots,x_{n-(m-1)}\}$.
    %
    The mean of such predictor is

    \begin{align*}
        \text{mean}(\text{pred}(x_{m+h}|x_n,\ldots,x_{n-(m-1)}))=\mu+\mathbf{a_m}^\intercal\left[\begin{array}{c}
                                                                                          x_n-\mu\\
                                                                                          \ldots\\
                                                                                          x_{n-(m-1)}-\mu
                                                                                      \end{array}\right]
    \end{align*}

    and its variance is

    \begin{align*}
        \text{var}(\text{pred}(x_{n+h}|x_n,\ldots,x_{n-(m-1)}))=\gamma(0)-\mathbf{a_m}^\intercal\gamma_m(h)
    \end{align*}

\end{frame}

\begin{frame}
    \frametitle{Forecasting}

    with
    \scriptsize
    \begin{align*}
        \Gamma_m\mathbf{a_m}&=\gamma_m(h)\\
        \Gamma_m&=[\gamma(i-j)]_{i,j=1}^m=\left[\begin{array}{c c c c c c}
                            \gamma(0) & \gamma(1) & \gamma(2) & \gamma(3) & \ldots & \gamma(m-1)\\
                            \gamma(1) & \gamma(0) & \gamma(1) & \gamma(2) & \ldots & \gamma(m-2)\\
                            \gamma(2) & \gamma(1) & \gamma(0) & \gamma(1) & \ldots & \gamma(m-3)\\
                            \vdots    & \vdots    & \vdots    & \vdots    & \vdots & \vdots\\
                            \gamma(m-1) & \gamma(m-2) & \gamma(m-3) & \gamma(m-4) & \ldots & \gamma(0)\\
                        \end{array}\right]\\
        \mathbf{a_m}&=[a_1,\ldots,a_m]^\intercal\\
        \gamma_m(h)&=[\gamma(h),\gamma(h+1),\ldots,\gamma(h+m-1)]^\intercal
    \end{align*}
    \normalsize

\end{frame}

\begin{frame}
    \frametitle{AR(p) forecasting example}

    \begin{exampleblock}{Example (Forecasting with an AR(p) model)}
        Simulate N=10,000 samples from an AR(7) stochastic process with
        $\phi=[5.0/6,-1.0/6,0.5/6,-0.25/6,0.5/6,-0.1/6,0.05/6]$ and
        $\sigma_w=5.0$. Use the last 500 samples to forecast 50 samples (i.e.,
        $n=10,000,m=500,h=1,\ldots,50$).

        \begin{center}
            \href{http://www.gatsby.ucl.ac.uk/~rapela/statNeuro/2025/lectures/01_temporalTimeSeriesAnalysis/figures/forecastingAR7.html}{\includegraphics[width=2.00in]{../../../solutions/worksheets/01_temporalTimeSeriesAnalysis/figures/forecastingAR7.png}}
        \end{center}
    \end{exampleblock}
\end{frame}

\begin{frame}
    \frametitle{Summary}

\end{frame}

\begin{frame}
    \frametitle{References}

	\begin{itemize}
		\item \citet{brockwellAndDavis02}
		\item \citet{shumwayAndStoffer16}
		\item \citet{priestley81}
	\end{itemize}

\end{frame}

% Then use
% Then use
\beginappendix
% after last slide and before first appendix slide, and then

\section{Appendix}

\begin{frame}
	\frametitle{MA($\infty$) representation of AR(1) random process}
	\label{slide:ar1AsMaInfty}

	\begin{claim}
		Let $|\phi|<1$, then
		\begin{align}
			x_t &= \phi x_{t-1} + w_t\quad\text{if and only if}\label{eq:ar}\\
			x_t &= \sum_{i=0}^\infty\phi^iw_{t-i}\label{eq:ma}
		\end{align}
	\end{claim}
\end{frame}

\begin{frame}
	\frametitle{MA($\infty$) representation of AR(1) random process}
	\small
    \begin{proof}
		\only<1> {
		We first show that $x_t$, as defined in Eq.~\ref{eq:ma}, satisfies Eq.~\ref{eq:ar}.
		\begin{align*}
			\phi x_{t-1}&=\phi\sum_{i=0}^\infty\phi^iw_{t-1-i}=\phi\sum_{j=1}^\infty\phi^{j-1}w_{t-j}=\sum_{j=1}^\infty\phi^jw_{t-j}\\
        	\phi x_{t-1}+w_t&=\sum_{j=0}^\infty\phi^jw_{t-j}=x_j
		\end{align*}
		}
		\only<2> {
        We now show that Eq.~\ref{eq:ma} is the unique solution to
        Eq.~\ref{eq:ar1}. Suppose $y_t$ is stationary and satisfies
        Eq.~\ref{eq:ar1}, then
		\begin{align*}
			y_t &= \phi y_{t-1}+w_t\\
			    &= \phi (\phi y_{t-2}+w_{t-1})+w_t=\phi^2y_{t-2}+\phi w_{t-1}+w_t\\
			    &= \phi^{t+1}y_{t-(t+1)}+\phi^tw_t+\ldots+\phi w_{t-1}+w_t\\
                &= \phi^{k+1}y_{t-(k+1)}+\sum_{i=0}^k\phi^kw_{t-i}\\
			E\left\{\left(y_t-\sum_{i=0}^k\phi^iw_{t-i}\right)^2\right\}&=\phi^{2k+2}E\{y_{t-(k+1)}^2\}=\phi^{2k+2}\sigma^2\\
			E\left\{\left(y_t-\sum_{i=0}^\infty\phi^iw_{t-i}\right)^2\right\}&=\lim_{k\rightarrow\infty}\phi^{2k+2}\sigma^2=0
		\end{align*}
		}
		\only<3> {
			Thus $y_t$ equals $\sum_{i=0}^\infty\phi^iw_{t-i}$ in the mean-squared sense.
		}
		\alt<3>{\qedhere}{\phantom\qedhere}
    \end{proof}
\end{frame}

\begin{frame}
	\frametitle{Alternative representation of the random walk with drift}
	\label{slide:alternativeRepRandomWalkWithDrif}

	\begin{claim}
        For $t=1,2,\ldots$, the random noise with drift model in Eq.~\ref{eq:randomWalkWithDrift}
        can be represented as
		\begin{align*}
            x_t &= \delta t +\sum_{i=1}^tw_i
		\end{align*}
	\end{claim}
\end{frame}

\begin{frame}
	\frametitle{Alternative representation of the random walk with drift}

	\scriptsize
    \begin{proof}
            We prove this claim by induction. We define a property $P_t$, we
            demonstrate the $P_1$ holds, and then we demonstrate that if $P_t$ holds
            then $P_{t+1}$ also holds. This strategy then demonstrates that $P_t$ holds
            for $t=1,2,\ldots$. Define
            \vspace{0.1in}
            \begin{align*}
                P_t:\quad x_t=\delta t+\sum_{i=1}^tw_i
            \end{align*}
            We first prove $P_1$
            \begin{align}
                P_1:\quad x_1=\delta+x_0+w_1=\delta+w_1\label{eq:P1}
            \end{align}
            Next we assume that $P_t$ holds and prove that $P_{t+1}$ also holds
            \begin{align}
                P_t\rightarrow P_{t+1}:\quad x_{t+1}&=\delta+x_t+w_{t+1}=\delta+\left(\delta t+\sum_{i=1}^tw_i\right)+w_{t+1}\label{eq:PtIPtp1}\\
                                                    &=\delta (t+1)+\sum_{i=1}^{t+1}w_i\qed\nonumber
            \end{align}
    \end{proof}

\end{frame}

\begin{frame}
	\frametitle{Alternative representation of the random walk with drift}

    \begin{block}{Note}
        \begin{enumerate}
            \item the first equality in Eq.~\ref{eq:P1} follows from the
                definition of the random walk with drift in
                Eq.~\ref{eq:randomWalkWithDrift}.
            \item the last equality in Eq.~\ref{eq:PtIPtp1} uses the
                hypothesized $P_t$.
        \end{enumerate}
    \end{block}
\end{frame}

\finishappendix
% after last appendix slide but before \end{document}

\bibliographystyle{apalike}
\bibliography{others,stats}

\end{document}
