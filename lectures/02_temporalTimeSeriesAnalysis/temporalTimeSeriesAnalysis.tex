
\documentclass{beamer}

\hypersetup{colorlinks=true}

\mode<presentation> {

% The Beamer class comes with a number of default slide themes
% which change the colors and layouts of slides. Below this is a list
% of all the themes, uncomment each in turn to see what they look like.

%\usetheme{default}
%\usetheme{AnnArbor}
%\usetheme{Antibes}
%\usetheme{Bergen}
%\usetheme{Berkeley}
%\usetheme{Berlin}
%\usetheme{Boadilla}
%\usetheme{CambridgeUS}
%\usetheme{Copenhagen}
%\usetheme{Darmstadt}
%\usetheme{Dresden}
%\usetheme{Frankfurt}
%\usetheme{Goettingen}
%\usetheme{Hannover}
%\usetheme{Ilmenau}
%\usetheme{JuanLesPins}
%\usetheme{Luebeck}
\usetheme{Madrid}
%\usetheme{Malmoe}
%\usetheme{Marburg}
%\usetheme{Montpellier}
%\usetheme{PaloAlto}
%\usetheme{Pittsburgh}
%\usetheme{Rochester}
%\usetheme{Singapore}
%\usetheme{Szeged}
%\usetheme{Warsaw}

% As well as themes, the Beamer class has a number of color themes
% for any slide theme. Uncomment each of these in turn to see how it
% changes the colors of your current slide theme.

%\usecolortheme{albatross}
%\usecolortheme{beaver}
%\usecolortheme{beetle}
%\usecolortheme{crane}
%\usecolortheme{dolphin}
%\usecolortheme{dove}
%\usecolortheme{fly}
%\usecolortheme{lily}
%\usecolortheme{orchid}
%\usecolortheme{rose}
%\usecolortheme{seagull}
%\usecolortheme{seahorse}
%\usecolortheme{whale}
%\usecolortheme{wolverine}

%\setbeamertemplate{footline} % To remove the footer line in all slides uncomment this line
%\setbeamertemplate{footline}[page number] % To replace the footer line in all slides with a simple slide count uncomment this line

%\setbeamertemplate{navigation symbols}{} % To remove the navigation symbols from the bottom of all slides uncomment this line
}

% remove title and author from left panel
%  \makeatletter
%   \setbeamertemplate{sidebar \beamer@sidebarside}%{sidebar theme}
%   {
%     \beamer@tempdim=\beamer@sidebarwidth%
%     \advance\beamer@tempdim by -6pt%
%     \insertverticalnavigation{\beamer@sidebarwidth}%
%     \vfill
%     \ifx\beamer@sidebarside\beamer@lefttext%
%     \else%
%       \usebeamercolor{normal text}%
%       \llap{\usebeamertemplate***{navigation symbols}\hskip0.1cm}%
%       \vskip2pt%
%     \fi%
%   }%
% \makeatother
% done remove title and author from left panel 

\usepackage{graphicx} % Allows including images
\usepackage{booktabs} % Allows the use of \toprule, \midrule and \bottomrule in tables
\usepackage{natbib}
\usepackage{apalike}
\usepackage{comment}
\usepackage{listings,lstautogobble}
% \usepackage{enumitem}
% \setlist[itemize]{topsep=0pt,before=\leavevmode\vspace{-1.5em}}
% \setlist[description]{style=nextline}
\usepackage{amsthm}
\usepackage{media9}
% \usepackage{multimedia}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{verbatim}
% \usepackage{hyperref}

% \usepackage{tikz}
% \tikzset{
%      arrow/.style={-{Stealth[]}}
%      }
% \usetikzlibrary{positioning,arrows.meta}
% \usetikzlibrary{shapes.geometric}

\setbeamertemplate{navigation symbols}{}%remove navigation symbols
\setbeamertemplate{caption}[numbered]%allow figure numbers

\usepackage{setspace}

% At the beginning put
%This custom command allows the number of frames to be stopped at this point
%(so later slides, eg. appendix do not appear as part of counter)
\newcommand{\beginappendix}{
   \newcounter{finalframenumber}
   \setcounter{finalframenumber}{\value{framenumber}}
}
\newcommand{\finishappendix}{
   \setcounter{framenumber}{\value{finalframenumber}}
}

\newtheorem{claim}{Claim}
\newtheorem{exercise}{Exercise}
\setbeamertemplate{theorems}[numbered]

\newenvironment<>{example1}[1][Example 1]{%
  \setbeamercolor{block title}{fg=white,bg=cyan!75!black}%
  \begin{block}{#1}}{\end{block}}
\newenvironment<>{example2}[1][Example 2]{%
  \setbeamercolor{block title}{fg=white,bg=magenta!75!black}%
  \begin{block}{#1}}{\end{block}}

\newcounter{saveenumi}
\newcommand{\seti}{\setcounter{saveenumi}{\value{enumi}}}
\newcommand{\conti}{\setcounter{enumi}{\value{saveenumi}}}
\newcommand{\keepi}{\addtocounter{saveenumi}{-1}\setcounter{enumi}{\value{saveenumi}}}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\title{Temporal Time Series Analysis (Part II)}

\author{Joaqu\'{i}n Rapela} % Your name
\institute[Gatsby Unit, UCL] % Your institution as it will appear on the bottom of every slide, may be shorthand to save space
{
Gatsby Computational Neuroscience Unit\\University College London % Your institution for the title page
}
\date{\today} % Date, can be changed to a custom date

\AtBeginSection[]
  {
     \begin{frame}<beamer>
     \frametitle{Contents}
         \tableofcontents[currentsection,hideallsubsections]
     \end{frame}
  }

\AtBeginSubsection[]
  {
     \begin{frame}<beamer>
     \frametitle{Contents}
         \tableofcontents[currentsection,currentsubsection]
     \end{frame}
  }

\begin{document}

\begin{frame}
\titlepage % Print the title page as the first slide
\end{frame}

\begin{frame}
\frametitle{Contents} % Table of contents slide, comment this block out to remove it
\tableofcontents % Throughout your presentation, if you choose to use \section{} and \subsection{} commands, these will automatically be printed on this slide as an overview of your presentation
\end{frame}

\section{Forecasting}

\begin{frame}
    \frametitle{Forecasting}

    Forecasting is the problem of predicting the value of $x_{n+h}, h>0$, of a
    stationary time series, in term of the previous $m$
    values$\{x_n,\ldots,x_{n-(m-1)}\}$.
    %
    The mean of such predictor is

    \begin{align*}
        \text{mean}(\text{pred}(x_{m+h}|x_n,\ldots,x_{n-(m-1)}))=\mu+\mathbf{a_m}^\intercal\left[\begin{array}{c}
                                                                                          x_n-\mu\\
                                                                                          \ldots\\
                                                                                          x_{n-(m-1)}-\mu
                                                                                      \end{array}\right]
    \end{align*}

    and its variance is

    \begin{align*}
        \text{var}(\text{pred}(x_{n+h}|x_n,\ldots,x_{n-(m-1)}))=\gamma(0)-\mathbf{a_m}^\intercal\gamma_m(h)
    \end{align*}

\end{frame}

\begin{frame}
    \frametitle{Forecasting}

    with
    \scriptsize
    \begin{align*}
        \Gamma_m\mathbf{a_m}&=\gamma_m(h)\\
        \Gamma_m&=[\gamma(i-j)]_{i,j=1}^m=\left[\begin{array}{c c c c c c}
                            \gamma(0) & \gamma(1) & \gamma(2) & \gamma(3) & \ldots & \gamma(m-1)\\
                            \gamma(1) & \gamma(0) & \gamma(1) & \gamma(2) & \ldots & \gamma(m-2)\\
                            \gamma(2) & \gamma(1) & \gamma(0) & \gamma(1) & \ldots & \gamma(m-3)\\
                            \vdots    & \vdots    & \vdots    & \vdots    & \vdots & \vdots\\
                            \gamma(m-1) & \gamma(m-2) & \gamma(m-3) & \gamma(m-4) & \ldots & \gamma(0)\\
                        \end{array}\right]\\
        \mathbf{a_m}&=[a_1,\ldots,a_m]^\intercal\\
        \gamma_m(h)&=[\gamma(h),\gamma(h+1),\ldots,\gamma(h+m-1)]^\intercal
    \end{align*}
    \normalsize

\end{frame}

\begin{frame}
    \frametitle{AR(1) forecasting example}

    \begin{exampleblock}{Example (Forecasting with an AR(1) model)}
        Simulate N=1,000 samples from an AR(1) stochastic process with
        $\phi=-0.9$ and
        $\sigma_w=1.0$. Use the last 500 samples to forecast 50 samples (i.e.,
        $n=1,000,m=500,h=1,\ldots,50$).
        \href{https://joacorapela.github.io/statNeuro2025/auto_examples/01_temporalTimeSeriesAnalysis/plot_ar1ForecastingTrueCor.html\#sphx-glr-auto-examples-01-temporaltimeseriesanalysis-plot-ar1forecastingtruecor-py}{Solution}.

        \begin{center}
            \href{http://www.gatsby.ucl.ac.uk/~rapela/statNeuro/2025/lectures/01_temporalTimeSeriesAnalysis/figures/forecastingAR1.html}{\includegraphics[width=2.00in]{../../examples/sphinx_gallery/01_temporalTimeSeriesAnalysis/figures/forecastingAR1.png}}
        \end{center}
    \end{exampleblock}
\end{frame}

\begin{frame}
    \frametitle{Marginals and conditionals of Gaussians are Gaussians}

	\small
	\begin{theorem}[Marginals and conditionals of Gaussians are Gaussians]
		\label{thm:marginalOrConditionalOfGaussianIsGaussian}

		Given $\mathbf{x}=\left[\begin{array}{c}
									\mathbf{x}_a\\
									\mathbf{x}_b\\
								\end{array}\right]$ such that

		\begin{align*}
			p(\mathbf{x})&=\mathcal{N}\left(\mathbf{x}\left|
				\left[\begin{array}{c}
					      \boldsymbol{\mu}_a\\
						  \boldsymbol{\mu}_b\\
				   	  \end{array}\right],
				\left[\begin{array}{cc}
					      \Sigma_{aa} & \Sigma_{ab}\\
						  \Sigma_{ba} & \Sigma_{bb}\\
					  \end{array}\right]
			\right.\right)\\
			            &=\mathcal{N}\left(\mathbf{x}\left|
				\left[\begin{array}{c}
					      \boldsymbol{\mu}_a\\
						  \boldsymbol{\mu}_b\\
				   	  \end{array}\right],
				\left[\begin{array}{cc}
					      \Lambda_{aa} & \Lambda_{ab}\\
						  \Lambda_{ba} & \Lambda_{bb}\\
					  \end{array}\right]^{-1}
			\right.\right)
		\end{align*}
		Then
		\begin{align}
			p(\mathbf{x}_a|\mathbf{x}_b)&=\mathcal{N}\left(\mathbf{x}_a\left|\boldsymbol{\mu}_a-\Lambda_{aa}^{-1}\Lambda_{ab}(\mathbf{x}_b-\boldsymbol{\mu}_b),\Lambda_{aa}^{-1}\right.\right)\label{eq:gaussianCond1}\\
			                            &=\mathcal{N}\left(\mathbf{x}_a\left|\boldsymbol{\mu}_a+\Sigma_{ab}\Sigma_{bb}^{-1}(\mathbf{x}_b-\boldsymbol{\mu}_b),\Sigma_{aa}-\Sigma_{ab}\Sigma_{bb}^{-1}\Sigma_{ba}\right.\right)\label{eq:gaussianCond2}\\
            p(\mathbf{x}_b)&=\mathcal{N}\left(\mathbf{x}_b\left|\boldsymbol{\mu}_b,\Sigma_{bb}\right.\right)\label{eq:gaussianMarginal}
		\end{align}

	\end{theorem}
    Proof in the \hyperlink{slide:proofCondGaussians}{Appendix}.
	\normalsize
\end{frame}

\begin{frame}
    \frametitle{Relevance of the conditional density of Gaussians}

	The expression of the conditional density of jointly Gaussian random
    variables is used in the derivation of

	\begin{enumerate}

		\item Bayesian linear regression~\citep{bishop06},

		\item Gaussian process regression~\citep{williamsAndRasmussen06},

		\item Gaussian process factor analysis~\citep{yuEtAl09},

		\item linear dynamical systems~\citep{durbinAndKoopman12}.

	\end{enumerate}

\end{frame}

\begin{frame}
    \frametitle{Derivation of the forecasting equations using the expression of the conditional of Gaussians}

    Take $\mathbf{x}_a=[x_{m+h}]$ and
    $\mathbf{x}_b=[x_n,\ldots,x_{n-m+1}]^\intercal$ in
    Eq.~\ref{eq:gaussianCond2}.

\end{frame}

\begin{frame}
    \frametitle{Derivation of estimator of missing values using the expression
    of the conditional of Gaussians}

    \begin{exercise}

        You are given an AR(1) time series with missing values
        $[x_{n+1},\dots,x_{n+h}]$.  Use the expression of the conditional of
        Gaussians to find the optimal estimator, in the mean square error
        sense, of the missing values using observations
        $[x_n,\ldots,x_{n-(m-1)}]$ and $[x_{n+h+1},\ldots,x_{n+h+m}]$.

    \end{exercise}

        Hint: take $\mathbf{x}_a=[x_{n+1},\dots,x_{n+h}]$ and
        $\mathbf{x}_b=[x_{n+h+1},\ldots,x_{n+h+m},x_n,\ldots,x_{n-(m-1)}]$
        Eq.~\ref{eq:gaussianCond2}.

\end{frame}

\section{Estimation of coefficients of AR(p) models using the Yule-Walker
equations}

\begin{frame}
    \frametitle{Yule-Walker equations}

    \small
    \begin{claim}[Yule-Walker equations for AR(p) model]
        If $\{x_t\}$ is and AR(p) random process

        \begin{align*}
            x_t=\phi_1x_{t-1}+\cdots+\phi_px_{t-p}+w_t\quad\text{with}\;w_t\sim N(0,\sigma^2)
        \end{align*}

        then

        \begin{align}
            \gamma(h)&=\phi_1\gamma(h-1)+\cdots+\phi_p\gamma(h-p)\quad h=1,\ldots,p\label{eq:ywh}\\
            \gamma(0)&=\phi_1\gamma(h-1)+\cdots+\phi_p\gamma(h-p)+\sigma^2\label{eq:yw0}
        \end{align}

    \end{claim}

    \begin{proof}
        See board.
    \end{proof}
    \normalsize
\end{frame}

\begin{frame}
    \frametitle{Yule-Walker equations}

    In matrix form the Yule-Walker equations~\ref{eq:ywh} and~\ref{eq:yw0} can
    be written as:

    \scriptsize
    \begin{align}
        \Gamma_p\phi&=\gamma_p\label{eq:ywPhi}\\
        \gamma(0)&=\phi^\intercal\gamma_p+\sigma^2\label{eq:ywSigma}\\
                 &\text{with}\nonumber\\
        \Gamma_p&=[\gamma(i-j)]_{i,j=1}^p=\left[\begin{array}{c c c c c c}
                            \gamma(0) & \gamma(1) & \gamma(2) & \gamma(3) & \ldots & \gamma(p-1)\\
                            \gamma(1) & \gamma(0) & \gamma(1) & \gamma(2) & \ldots & \gamma(p-2)\\
                            \gamma(2) & \gamma(1) & \gamma(0) & \gamma(1) & \ldots & \gamma(p-3)\\
                            \vdots    & \vdots    & \vdots    & \vdots    & \vdots & \vdots\\
                            \gamma(p-1) & \gamma(p-2) & \gamma(p-3) & \gamma(p-4) & \ldots & \gamma(0)\\
                        \end{array}\right]\nonumber\\
        \phi&=[\phi(1),\ldots,\phi(p)]^\intercal\nonumber\\
        \gamma_p&=[\gamma(1),\ldots,\gamma(p)]^\intercal\nonumber
    \end{align}
    \normalsize

\end{frame}

\begin{frame}
    \frametitle{Yule-Walker estimators}

    Replacing $\gamma$ by its estimate $\hat\gamma$ in Eqs.~\ref{eq:ywPhi}
    and~\ref{eq:ywSigma}, we obtain the Yule-Walker estimators

    \begin{align*}
        \hat\phi&=\hat\Gamma_p^{-1}\hat\gamma_p(h)\\
        \hat\sigma^2&=\hat\gamma(0)-\hat\phi^\intercal\hat\gamma_p(h)\\
                 &\text{with}\\
        \hat\Gamma_p&=[\hat\gamma(i-j)]_{i,j=1}^p\\
        \hat\phi&=[\hat\phi(1),\ldots,\hat\phi(p)]^\intercal\\
        \hat\gamma_p(h)&=[\hat\gamma(h-1),\ldots,\hat\gamma(h-p)]^\intercal
    \end{align*}

\end{frame}

\begin{frame}
    \frametitle{Large-sample distribution of Yule-Walker estimators}

    \begin{theorem}[Large-sample distribution of Yule-Walker estimators]
        For a large sample from an AR(p) random process

        \begin{align*}
            \hat\phi\sim N\left(\phi,n^{-1}\sigma^2\Gamma_p^{-1}\right).
        \end{align*}

    \end{theorem}
    \begin{proof}
        See~\citet[][Section 8.10]{brockwellAndDavis91}
    \end{proof}
\end{frame}

\begin{frame}
    \frametitle{Estimate coefficients of AR(3) model using the Yule-Walker estimators}

    Sample a time series of length $N=1000$ from an AR(3) model. Estimate the
    coefficients of this model, and the variance of the noise, using the
    Yule-Walker estimators. Also, calculate the large sample estimates of the
    coefficients' variance. Plot the true and estimated coefficients. Add a
    95\% confidence bounds to the estimated coefficients.

    \begin{center}
        \href{http://www.google.com}{\includegraphics[width=2.5in]{../../../mySolutions/01_temporalTimeSeriesAnalysis/figures/coefficientsAR3N1000.png}}
    \end{center}

\end{frame}

\section{Likelihood function (for the estimation of coefficients)}

\begin{frame}
    \frametitle{Likelihood function}

    \begin{definition}[Likelihood function]

    Consider a random process $\{x_t\}$ with a probability density function
        parameterised by parameters $\theta$, $f(\{x_t\}|\theta)$.  Given a
        sample $\{x_i\}_{i=1}^N$, the \textbf{likelihood function} of $\theta$,
        $\mathcal{L}(\theta)$, assigns to $\theta$ the value $f(\{x_i\}_{i=1}^N|\theta)$.

    \end{definition}

\end{frame}

\begin{frame}
    \frametitle{Likelihood function}

    \begin{claim}[Likelihood function for an AR(1) random process]
        The log likelihood function for the parameters
        $\theta=\{\phi,\sigma^2\}$ of an AR(1) random process, given
        observations $\{x_1,\ldots,x_N\}$ is

        \begin{align*}
            \log\mathcal{L}(\phi,\sigma^2)=&-\frac{N-1}{2}\log(2\pi\sigma^2)-\frac{1}{2\sigma^2}\sum_{n-2}^N(x_n-\phi x_{n-1})^2\\
                                           &-\frac{1}{2}\log(2\pi\gamma(0))-\frac{x_i^2}{2\gamma(0)}
        \end{align*}

    \end{claim}

    \begin{proof}[Likelihood function for an AR(1) random process]
        See board
    \end{proof}

\end{frame}

\begin{frame}
    \frametitle{Maximum likelihood parameter estimates}

    \begin{definition}[Maximum likelihood parameters estimates]
        Given a data sample $\{x_t\}$, the \textbf{maximum likelihood
        parameters estimates} are $\hat\theta_{ML}=\argmax_\theta\mathcal{L}(\theta)$.
    \end{definition}

\end{frame}

\begin{frame}
    \frametitle{Maximum likelihood estimates of parameters of AR(1) process}

    \begin{example}
        Simulate a time series of length $N=10,000$ from an AR(1) random
        process with $\phi=0.3$ and $\sigma=1$. Calculate the log-likelihood
        function on the simulated time series in the grid of parameters
        $0.85\le\sigma\le 1.10$ (spacing $\delta_\sigma=0.01$ and
        $-0.95\le\phi\le 0.95$ (spacing $\delta_\phi=0.05$. Verify that the
        calculated log likelihood is maximised at the simulated parameter
        values.
    \end{example}

    \begin{center}
        \href{http://www.google.com}{\includegraphics[width=2.0in]{../../../mySolutions/01_temporalTimeSeriesAnalysis/figures/logLikesAR1.png}}
    \end{center}

\end{frame}

\begin{frame}
    \frametitle{Maximum likelihood estimates of parameters of AR(1) process}

    \begin{center}
        \href{http://www.google.com}{\includegraphics[width=2.0in]{../../../mySolutions/01_temporalTimeSeriesAnalysis/figures/logLikesAR1_phi.png}}
    \end{center}

    \begin{center}
        \href{http://www.google.com}{\includegraphics[width=2.0in]{../../../mySolutions/01_temporalTimeSeriesAnalysis/figures/logLikesAR1_sigma.png}}
    \end{center}

\end{frame}

\begin{frame}
    \frametitle{Summary}

\end{frame}

\begin{frame}
    \frametitle{References}

    \textbf{time series analysis}
        \begin{itemize}
            \item \citet{brockwellAndDavis02}
            \item \citet{shumwayAndStoffer16}
            \item \citet{priestley81}
     \end{itemize}

    \textbf{machine learning}
        \begin{itemize}
            \item \citet{bishop06}
            \item \citet{murphyIntro22}
        \end{itemize}

\end{frame}

% Then use
% Then use
\beginappendix
% after last slide and before first appendix slide, and then

\section{Appendix}

\begin{frame}
    \frametitle{Proof: the conditional of a Gaussian is a Gaussian (Theorem~\ref{thm:marginalOrConditionalOfGaussianIsGaussian}, Eq.~\ref{eq:gaussianCond1})}
    \label{slide:proofCondGaussians}

	\begin{claim}[Quadratic form of Gaussian log pdf]
		\label{claim:quadratricFormOfGaussianPDF}

		$p(\mathbf{x})$ is a Gaussian pdf with mean $\boldsymbol{\mu}$ and precision matrix $\Lambda$ if and only if $\int p(\mathbf{x}) d\mathbf{x}=1$ and

		\begin{align}
			\log p(\mathbf{x})=-\frac{1}{2}(\mathbf{x}^\intercal\Lambda\mathbf{x}-2\mathbf{x}^\intercal\Lambda\boldsymbol{\mu})+K\label{eq:gaussianQuadratic}
		\end{align}

		where $K$ is a constant that does not depend on $\mathbf{x}$.

	\end{claim}

\end{frame}

\begin{frame}
    \frametitle{Proof: the conditional of a Gaussian is a Gaussian (Theorem~\ref{thm:marginalOrConditionalOfGaussianIsGaussian}, Eq.~\ref{eq:gaussianCond1})}

	\begin{proof}[Proof of Claim~\ref{claim:quadratricFormOfGaussianPDF}]

		\scriptsize
		\begin{description}
			\item[$\rightarrow)$]

				\begin{align*}
					p(\mathbf{x})&=\frac{1}{(2\pi)^{D/2}\Lambda^{-\frac{1}{2}}}\exp\left\{-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^\intercal\Lambda(\mathbf{x}-\boldsymbol{\mu})\right\}\\
					\log p(\mathbf{x})&=-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^\intercal\Lambda(\mathbf{x}-\boldsymbol{\mu})-\log ((2\pi)^{D/2}\Lambda^{-\frac{1}{2}})\\
					                  &=-\frac{1}{2}(\mathbf{x}^\intercal\Lambda\mathbf{x}-2\mathbf{x}^\intercal\Lambda\boldsymbol{\mu})-\frac{1}{2}\boldsymbol{\mu}^\intercal\Lambda\boldsymbol{\mu}-\log ((2\pi)^{D/2}\Lambda^{-\frac{1}{2}})\\
					                  &=-\frac{1}{2}(\mathbf{x}^\intercal\Lambda\mathbf{x}-2\mathbf{x}^\intercal\Lambda\boldsymbol{\mu})+K
				\end{align*}
				with $K=-\frac{1}{2}\boldsymbol{\mu}^\intercal\Lambda\boldsymbol{\mu}-\log ((2\pi)^{D/2}\Lambda^{-\frac{1}{2}})$.
				\phantom\qedhere
		\end{description}
		\normalsize
	\end{proof}
\end{frame}

\begin{frame}
    \frametitle{Proof: the conditional of a Gaussian is a Gaussian (Theorem~\ref{thm:marginalOrConditionalOfGaussianIsGaussian}, Eq.~\ref{eq:gaussianCond1})}

	\begin{proof}[Proof of Claim~\ref{claim:quadratricFormOfGaussianPDF}]

		\scriptsize
		\begin{description}
			\item[$\leftarrow)$]

				\begin{align}
					\log p(\mathbf{x})=&-\frac{1}{2}(\mathbf{x}^\intercal\Lambda\mathbf{x}-2\mathbf{x}^\intercal\Lambda\boldsymbol{\mu})+K\nonumber\\
					\log p(\mathbf{x})=&-\frac{1}{2}(\mathbf{x}^\intercal\Lambda\mathbf{x}-2\mathbf{x}^\intercal\Lambda\boldsymbol{\mu})-\frac{1}{2}\boldsymbol{\mu}^\intercal\Lambda\boldsymbol{\mu}-\log ((2\pi)^{D/2}\Lambda^{-\frac{1}{2}})\nonumber\\
					                   &+K+\frac{1}{2}\boldsymbol{\mu}^\intercal\Lambda\boldsymbol{\mu}+\log ((2\pi)^{D/2}\Lambda^{-\frac{1}{2}})\nonumber\\
					                  =&-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^\intercal\Lambda(\mathbf{x}-\boldsymbol{\mu})-\log ((2\pi)^{D/2}\Lambda^{-\frac{1}{2}})\nonumber\\
					                   &+K+\frac{1}{2}\boldsymbol{\mu}^\intercal\Lambda\boldsymbol{\mu}+\log ((2\pi)^{D/2}\Lambda^{-\frac{1}{2}})\nonumber\\
					                  =&\log N(\mathbf{x}|\boldsymbol{\mu},\Lambda)+K+\frac{1}{2}\boldsymbol{\mu}^\intercal\Lambda\boldsymbol{\mu}+\log ((2\pi)^{D/2}\Lambda^{-\frac{1}{2}})\nonumber\\
					     p(\mathbf{x})=&N(\mathbf{x}|\boldsymbol{\mu},\Lambda)\exp\left(K+\frac{1}{2}\boldsymbol{\mu}^\intercal\Lambda\boldsymbol{\mu}+\log ((2\pi)^{D/2}\Lambda^{-\frac{1}{2}})\right)\label{eq:quadratricFormOfGaussianPDF_almostFinal}
				\end{align}
				\phantom\qedhere
		\end{description}
		\normalsize
	\end{proof}
\end{frame}

\begin{frame}
    \frametitle{Proof: the conditional of a Gaussian is a Gaussian (Theorem~\ref{thm:marginalOrConditionalOfGaussianIsGaussian}, Eq.~\ref{eq:gaussianCond1})}

	\begin{proof}[Proof of Claim~\ref{claim:quadratricFormOfGaussianPDF}]

		\scriptsize
		\begin{description}
			\item[$\leftarrow)$ cont]
				\begin{align*}
					1&=\int p(\mathbf{x})d\mathbf{x}\\
					 &=\int N(\mathbf{x}|\boldsymbol{\mu},\Lambda)\exp\left(K+\frac{1}{2}\boldsymbol{\mu}^\intercal\Lambda\boldsymbol{\mu}+\log ((2\pi)^{D/2}\Lambda^{-\frac{1}{2}})\right)d\mathbf{x}\\
					 &=\exp\left(K+\frac{1}{2}\boldsymbol{\mu}^\intercal\Lambda\boldsymbol{\mu}+\log ((2\pi)^{D/2}\Lambda^{-\frac{1}{2}})\right)\int N(\mathbf{x}|\boldsymbol{\mu},\Lambda)d\mathbf{x}\\
					 &=\exp\left(K+\frac{1}{2}\boldsymbol{\mu}^\intercal\Lambda\boldsymbol{\mu}+\log ((2\pi)^{D/2}\Lambda^{-\frac{1}{2}})\right)
				\end{align*}
				From Eq.~\ref{eq:quadratricFormOfGaussianPDF_almostFinal} then $p(\mathbf{x})=N(\mathbf{x}|\boldsymbol{\mu},\Lambda)$.
		\end{description}
		\normalsize
	\end{proof}
\end{frame}

\begin{frame}
    \frametitle{Proof: the conditional of a Gaussian is a Gaussian (Theorem~\ref{thm:marginalOrConditionalOfGaussianIsGaussian}, Eq.~\ref{eq:gaussianCond1})}

	\begin{proof}[Proof of Theorem~\ref{thm:marginalOrConditionalOfGaussianIsGaussian}, Eq.~\ref{eq:gaussianCond1}]

		\scriptsize
		\begin{align*}
			p(\mathbf{x}_a|\mathbf{x}_b)&=\frac{p(\mathbf{x}_a,\mathbf{x}_b)}{p(\mathbf{x}_b)}=\frac{p(\mathbf{x})}{p(\mathbf{x}_b)}\\
			\log p(\mathbf{x}_a|\mathbf{x}_b)&=\log p(\mathbf{x})-\log p(\mathbf{x}_b)=\log p(\mathbf{x})+K
		\end{align*}

		Therefore, the terms of $\log p(\mathbf{x}_a|\mathbf{x}_b)$ that depend on $\mathbf{x}_a$ are those of $\log p(\mathbf{x})$.

		Steps for the proof:

		\begin{enumerate}
			\item isolate the terms of $\log p(\mathbf{x})$ that depend on $\mathbf{x}_a$,
			\item notice that these term has the quadratic form of Claim~\ref{claim:quadratricFormOfGaussianPDF}, therefore $p(\mathbf{x}_a|\mathbf{x}_b)$ is Gaussian,
			\item identify $\boldsymbol{\mu}$ and $\Lambda$ in this quadratic form.
		\end{enumerate}

		\phantom\qedhere
		\normalsize
	\end{proof}
\end{frame}

\begin{frame}
    \frametitle{Proof: the conditional of a Gaussian is a Gaussian (Theorem~\ref{thm:marginalOrConditionalOfGaussianIsGaussian}, Eq.~\ref{eq:gaussianCond1})}

	\begin{proof}[Proof of Theorem~\ref{thm:marginalOrConditionalOfGaussianIsGaussian}, Eq.~\ref{eq:gaussianCond1}]

		\scriptsize
		\begin{align*}
			p(\mathbf{x})&=\frac{1}{(2\pi)^{D/2}|\Lambda|^{1/2}}\exp\left(-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^\intercal\Lambda(\mathbf{x}-\boldsymbol{\mu})\right)\\
			\log p(\mathbf{x})=&-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^\intercal\Lambda(\mathbf{x}-\boldsymbol{\mu})+K_1\\
			                  =&-\frac{1}{2}[(\mathbf{x}_a-\boldsymbol{\mu}_a)^\intercal,(\mathbf{x}_b-\boldsymbol{\mu}_b)^\intercal]\left[\begin{array}{cc}
							                                                                                                                   \Lambda_{aa} & \Lambda_{ab}\\
							                                                                                                                   \Lambda_{ba} & \Lambda_{bb}
																																	       \end{array}\right]
																																	 \left[\begin{array}{c}
																																	           \mathbf{x}_a-\boldsymbol{\mu}_a\\
																																	           \mathbf{x}_b-\boldsymbol{\mu}_b\\
																																			\end{array}\right]
																																	 +K_1\\
			                  =&-\frac{1}{2}\left\{(\mathbf{x}_a-\boldsymbol{\mu}_a)^\intercal\Lambda_{aa}(\mathbf{x}_a-\boldsymbol{\mu}_a)+2(\mathbf{x}_a-\boldsymbol{\mu}_a)^\intercal\Lambda_{ab}(\mathbf{x}_b-\boldsymbol{\mu}_b)\right.\\
							   &\left.+(\mathbf{x}_b-\boldsymbol{\mu}_b)^\intercal\Lambda_{bb}(\mathbf{x}_b-\boldsymbol{\mu}_b)\right\}+K_1\\
			                  =&-\frac{1}{2}\left\{\mathbf{x}_a^\intercal\Lambda_{aa}\mathbf{x}_a-2\mathbf{x}_a^\intercal(\Lambda_{aa}\boldsymbol{\mu}_a-\Lambda_{ab}(\mathbf{x}_b-\boldsymbol{\mu}_b))\right\}+K_2\\
			                  =&-\frac{1}{2}\left\{\mathbf{x}_a^\intercal\Lambda_{aa}\mathbf{x}_a-2\mathbf{x}_a^\intercal\Lambda_{aa}(\boldsymbol{\mu}_a-\Lambda_{aa}^{-1}\Lambda_{ab}(\mathbf{x}_b-\boldsymbol{\mu}_b))\right\}+K_2
		\end{align*}
		Comparing the last equation with Eq.~\ref{eq:gaussianQuadratic} we see that $\Lambda=\Lambda_{aa}$, $\boldsymbol{\mu}=\boldsymbol{\mu}_a-\Lambda_{aa}^{-1}\Lambda_{ab}(\mathbf{x}_b-\boldsymbol{\mu}_b)$ and conclude that $p(\mathbf{x}_a|\mathbf{x}_b)=\mathcal{N}(\mathbf{x}_a|\boldsymbol{\mu}_a-\Lambda_{aa}^{-1}\Lambda_{ab}(\mathbf{x}_b-\boldsymbol{\mu}_b),\Lambda_{aa})$
		\normalsize
	\end{proof}
\end{frame}

\begin{frame}
    \frametitle{Proof: the conditional of a Gaussian is a Gaussian (Theorem~\ref{thm:marginalOrConditionalOfGaussianIsGaussian}, Eq.~\ref{eq:gaussianCond2})}

	\begin{claim}[Inverse of a partitioned matrix]
		\begin{align}
			\left(\begin{array}{cc}
				      A & B\\
				      C & D
				  \end{array}\right)^{-1}=
			\left(\begin{array}{cc}
				      M         & -MBD^{-1}\\
					  -D^{-1}CM & D^{-1} + D^{-1}CMBD^{-1}
				  \end{array}\right)\label{eq:inversePartitionedMatrix}
		\end{align}
		where
		\begin{align*}
			M = (A - BD^{-1}C)^{-1}
		\end{align*}
	\end{claim}
	\begin{proof}
		\scriptsize
		Exercise. Hint: verify that the multiplication of the inverse of the matrix in the right hand side of Eq.~\ref{eq:inversePartitionedMatrix} with the matrix in the left hand side of the same equation is the identity matrix.
		\phantom\qedhere
		\normalsize
	\end{proof}
\end{frame}

\begin{frame}
    \frametitle{Proof: the conditional of a Gaussian is a Gaussian (Theorem~\ref{thm:marginalOrConditionalOfGaussianIsGaussian}, Eq.~\ref{eq:gaussianCond2})}

	\begin{proof}[Proof of Theorem~\ref{thm:marginalOrConditionalOfGaussianIsGaussian}, Eq.~\ref{eq:gaussianCond2}]
		\scriptsize
		Using the definition
		\begin{align*}
			\left(\begin{array}{cc}
				      \Sigma_{aa} & \Sigma_{ab}\\
				      \Sigma_{ba} & \Sigma_{bb}
                  \end{array}\right)^{-1}=
			\left(\begin{array}{cc}
				      \Lambda_{aa} & \Lambda_{ab}\\
				      \Lambda_{ba} & \Lambda_{bb}
                  \end{array}\right)
		\end{align*}
		and using Eq.~\ref{eq:inversePartitionedMatrix}, we obtain
		\begin{align*}
			\Lambda_{aa}&=(\Sigma_{aa}-\Sigma_{ab}\Sigma_{bb}^{-1}\Sigma_{ba})^{-1}\\
			\Lambda_{ab}&=-(\Sigma_{aa}-\Sigma_{ab}\Sigma_{bb}^{-1}\Sigma_{ba})^{-1}\Sigma_{ab}\Sigma_{bb}^{-1}
		\end{align*}
		Replacing the above equations in Eq.~\ref{eq:gaussianCond1} we obtain Eq.~\ref{eq:gaussianCond2}.
		\normalsize
	\end{proof}

\end{frame}

\finishappendix
% after last appendix slide but before \end{document}

\bibliographystyle{apalike}
\bibliography{others,stats,machinelearning,gaussianProcesses,latentsVariablesModels,linearDynamicalSystems}

\end{document}
